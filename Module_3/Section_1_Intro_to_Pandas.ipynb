{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rgFQ0jMkuMa8"
      },
      "source": [
        "# Section 1. Intro to Pandas\n",
        "\n",
        "So far, we have used Python lists as well as Numpy arrays to store and manipulate data, each of which have their place in a simple Python program. But there's an excellent (and very popular) Python package called Pandas that greatly facilitates handling comma separated value (CSV) files and other types of data files. The basic tool in Pandas is the DataFrame, which you can think of as a large table with built-in functions to process rows and columns, read and write to many different formats on disk, and interact with other DataFrames. One key advantage of pandas over other types of data storage is that it can easily handle data of many different types, including strings and numbers.\n",
        "\n",
        "## 1.1 Getting Started\n",
        "\n",
        "First things first, we need to import the package. We should also import NumPy for other useful functions and matplotlib for when we plot\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdPe93y5uB-r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DhOD9Xto_DNx"
      },
      "source": [
        "### 1.1.1 The DataFrame\n",
        "\n",
        "There are many ways to create DataFrames in Pandas. Here, we will use one method that involves lists. In this case, each row in the DataFrame starts as its own list, much like with a NumPy 2D array.\n",
        "\n",
        "First, let's define some familiar looking Solar System data.\n",
        "\n",
        "```\n",
        "planets = [['Mercury', 0.0553, 0.4, 0],\n",
        "           ['Venus', 0.815, 0.7, 0],\n",
        "           ['Earth', 1.0, 1.0, 1],\n",
        "           ['Mars', 0.107, 1.5, 2],\n",
        "           ['Jupiter', 317.8, 5.2, 80],\n",
        "           ['Saturn', 95.2, 9.5, 83],\n",
        "           ['Uranus', 14.5, 19.2, 27],\n",
        "           ['Neptune', 17.1, 30.1, 14]]\n",
        "\n",
        "colnames = ['Name','Mass','Distance','N_moons']\n",
        "# Units: None, Earth Masses, AU, None\n",
        "```\n",
        "\n",
        "Take note of a few things here. \n",
        "\n",
        "First, unlike in NumPy arrays, where you can only have one data type (usually int or float), each row in the above table mixes three different types of data: string, float, and int. This is one of the advantages of Pandas: different columns are allowed (and encouraged) to have different datatypes, depending on your needs.\n",
        "\n",
        "Second, notice that the code above uses a second list to create names for the columns. This is a huge feature of the design philosophy of the DataFrame. While a NumPy 2D array can be thought of as a huge matrix of numbers, a DataFrame really is a table. \n",
        "\n",
        "Finally, note that code also included a comment with the units associated with this data. Units were not included in the column names to keep the column names short, which helps writing the code later on be much easier. As a reminder, it is good practice to label units in your code with comments to stay organized and to be able to share your code with others so they can easily understand it.\n",
        "\n",
        "Now, let's make a dataframe named `df` from the lists named `planets` and print it out.\n",
        "```\n",
        "df = pd.DataFrame(planets, columns=colnames)\n",
        "print(df)\n",
        "\n",
        "```\n",
        "In the second cell, try running just `df` to see how the output may look different. This distinction is only relevant when working in a Jupyter notebook and when datasets are small enough to read and look over in Python.\n",
        "```\n",
        "df\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DD_ASdZXEnUy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUlfOT4_gV8E"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4XVvbgfhsk"
      },
      "source": [
        "### 1.1.2 Accessing Rows and Cells\n",
        "\n",
        "First, let's talk about how to access individual rows and cells. For this, we will use the method `df.loc`, where you can replace df with the name of your dataframe. \n",
        "\n",
        "You probably noticed in the previous example that each row is specified by an index. In the example above, we just had integer indices from 0 to 7. So, if we want to access the row for Earth, we can just execute\n",
        "```\n",
        "df.loc[2]\n",
        "```\n",
        "Note that some Pandas DataFrames you encounter may be indexed differently. For instance, the indices may not correspond to their row number at all. In fact, you can even find tables where the indices are strings. When using `df.loc` you must remember to use the label given.\n",
        "\n",
        "We can illustrate this quite well by showing how to access a specific cell. We simply specify both the index (as above) and the column name. So, if we wanted to get the mass of Venus, for example, we'd execute\n",
        "```\n",
        "df.loc[2,'Mass']\n",
        "```\n",
        "Try this yourself below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYPeOdeciGKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sRxCRggdiZbk"
      },
      "source": [
        "Note that we can also use this functionality with `.loc` to \n",
        "change the values of individual cells, but in practice when working with data we didn't create ourselves, there is no reason to do so.\n",
        "\n",
        "### 1.1.3 Accessing Columns\n",
        "\n",
        "Accessing individual rows and cells is actually less useful than you'd think since each row only represents one data point. Usually, when we're using tables, we're actually more interested in the statistics of many data points. The DataFrame structure represents this, because it's very easy to get different columns. For example, if we want the different masses,\n",
        "```\n",
        "df['Mass']\n",
        "```\n",
        "If we wanted multiple columns, we enter a list of column names instead of just one, e.g.,\n",
        "```\n",
        "df[['Mass','Distance']]\n",
        "```\n",
        "\n",
        "Note that this is why it is best to keep column names short by including units in a comment rather than in the column name as mentioned above. Since we need to write out the column names, adding the units can make the code harder to parse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evfVfW2CiN92"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KtFBqIS7po2g"
      },
      "source": [
        "### 1.1.4 df.values\n",
        "When using some functions, like `plt.plot()`, we don't want the full Pandas DataFrame. Using `df.values` will extract the values and return a NumPy array.\n",
        "```\n",
        "df.values # returns a 2D array\n",
        "df.loc[2].values # returns a 1D array of the third row\n",
        "df['Mass'].values # returns a 1D array of the Mass column\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9I5z5sGr1he"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_jFSOPt0r0oE"
      },
      "source": [
        "## 1.2 Making Changes to a DataFrame\n",
        "### 1.2.1 Adding New Columns from Data\n",
        "\n",
        "Let's say we get more data from a different source. We want to add it to our DataFrame. We can do so like this.\n",
        "```\n",
        "radii = [0.383, 0.949, 1, 0.532, 11.21, 9.45, 4.01, 3.88] # in Earth radii \n",
        "\n",
        "df['Radius'] = radii\n",
        "```\n",
        "\n",
        "One big note: **your list or list-like object with the new data must be in the same order as the rows in your DataFrame. If they aren't, your data will be incorrect. This is extremely important.**\n",
        "\n",
        "Add this new column below. It will be used in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el9h05K5sRGl"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OnyUwre6sRp2"
      },
      "source": [
        "### 1.2.2 Adding New Columns from Other Columns\n",
        "\n",
        "It's important to remember that columns are essentially NumPy arrays with extra stuff. As a result, we can do mathematical operations on columns just like we do with arrays. We can leverage this to create new columns. For example, we want to create a new column with the densities of each planet. Density is the ratio of the mass of an object over its volume, and we can calculate the densities by using the formula d = m / v.  Our DataFrame contains planetary mass in Earth masses and radius data in Earth radius. As a result, we'll convert the masses and radii to SI units, calculate the densities using those values, and add a column named `Density` to the DataFrame `df`.\n",
        "```\n",
        "m_e = 5.9722e24 # kg\n",
        "r_e = 6.371e6 # m, using the average radius\n",
        "df['Density'] = m_e*df['Mass']/((4/3)*np.pi*(r_e*df['Radius'])**3)\n",
        "```\n",
        "\n",
        "Now that you've done that, what do you notice about the densities of the outer planets versus the inner planets? Compare these densities to the densities of various materials in this [wikipedia page](https://en.wikipedia.org/wiki/Density#Densities). Which materials are these densities most similar to?\n",
        "\n",
        "What you should find is that the inner planets are rocky, so their densities are more comparable to metals, while the gas giants have densities that are closer to that of water."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQvBYew5Y2g"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KTndNlqg0b1r"
      },
      "source": [
        "### 1.2.3 Practice Problem 1: Kepler's Law\n",
        "\n",
        "Using Kepler's Third Law, make another column that corresponds to the orbital period for each planet **in years.** Once again, Keplers Third Law is \n",
        "\n",
        "$$T = 2\\pi \\sqrt{\\frac{a^3}{G(M_1 + M_2)}}$$\n",
        "\n",
        "For the case of solar system planets, you may make the approximation that $M_1 + M_2 \\approx M_\\odot \\approx 2e30$ kg. \n",
        "\n",
        "You can calculate by hand how many seconds are in a year, but note a year technically has 365.25 days!\n",
        "\n",
        "Finally, you'll need to know that 1 AU is defined as exactly 149597870700 m."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMpWTjUhDBzl"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N-weF6RUDCqV"
      },
      "source": [
        "### 1.2.4 Practice Problem 2: Equilibrium Temperature\n",
        "\n",
        "The [equilibrium temperature](https://en.wikipedia.org/wiki/Planetary_equilibrium_temperature) of a planet is defined as the temperature at which the power supplied by its star (due to solar radiation) is equal to the power emitted by the planet (due to blackbody radiation). Without getting too much into the details, the formula (in units of Kelvin) is as follows:\n",
        "\n",
        "$$ T_{\\rm eq} =  \\left( \\frac{L (1 - A_B)}{16\\sigma\\pi a^2} \\right)^{1/4}$$\n",
        "\n",
        "Here, L is the luminosity of the Sun (3.828e26 Watts), a is the semimajor axis (same as the \"distance\" column), $\\sigma =$ 5.67e-8 (SI units) is the Stefan-Boltzmann constant, and $A_B$ is something called the [Bond Albedo](https://en.wikipedia.org/wiki/Bond_albedo).\n",
        "\n",
        "Your task:\n",
        "1. Create a new column for the Bond albedo of each planet, taking the data from the linked wikipedia article\n",
        "2. Create a new column for the equilibrium temperature using the formula above. \n",
        "3. Create a new column for the effective/surface temperature of each planet using the values listed below. \n",
        "4. Create a scatter plot of the predicted equilibrium temperature versus the actual surface temperature. Show the 1-to-1 line (that is, the line y=x) using a dotted line for comparison. Make sure to restrict the range of this dotted line so that your data is still easily interpretable! After reading the first linked wikipedia article, can you think of at least one reason why they may be different? (Hint: you've definitely learned about one of them in other science classes or the news)\n",
        "\n",
        "```\n",
        "Teff = [412.5, 737, 288, 215, 124.4, 95, 59.1, 59.3]\n",
        "```\n",
        "Values taken from de Pater and Lissauer (2010). Note Mercury's is the average of 100 and 725\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quCOQWQmI_HI"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-yQ-Rr3-I_cX"
      },
      "source": [
        "## 1.3 Intermediate Pandas\n",
        "\n",
        "### 1.3.1 Boolean Indexing with DataFrames\n",
        "\n",
        "When we take a look at data, it can happen that we are only interested in a subset of the rows based on one or more conditions. Luckily for us, this is almost exactly identical to doing the same thing with NumPy arrays! The difference is that we use the column names instead of their indices. For example, let's get only the planets that have moons.\n",
        "\n",
        "```\n",
        "df[df['N_moons'] > 0]\n",
        "```\n",
        "\n",
        "Just like before, we can chain different things together. For example, let's get only the planets that have moons but are further than 5 au from the Sun.\n",
        "\n",
        "```\n",
        "df[(df['N_moons'] > 0) & (df['Distance'] > 5)]\n",
        "```\n",
        "\n",
        "As a reminder, here is a list of the logical operators you can use:\n",
        "\n",
        "```\n",
        "== # equal to\n",
        "!= # not equal to\n",
        "<= # less than or equal to\n",
        "<  # less than\n",
        ">= # greater than or equal to\n",
        ">  # greater than\n",
        "\n",
        "& # logical and\n",
        "| # logical or\n",
        "~ # logical not\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gFDgRL4UEAD"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5KomXNChLeix"
      },
      "source": [
        "### 1.3.2 df.iterrows()\n",
        "\n",
        "There are extremely rare cases where you will find it is necessary to iterate over the rows of a dataframe. These cases should be extremely rare. In almost every instance, there is some way of doing this using vectorized functions. In fact, the official documentation has a warning against doing this:\n",
        "\n",
        "> Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].\n",
        "\n",
        "In any case, there are times when this may be necessary. In those cases, we can use the function `df.iterrows()` which allows us to iterate over all the rows in the dataframe. In the following example\n",
        "```\n",
        "for i,row in df.iterrows():\n",
        "    print(row['Name'],'has',row['N_moons'],'moons.')\n",
        "```\n",
        "In this example, `df.iterrows()` returns an index,series pair for each row in the dataframe. Since each row is returned as a Pandas Series, we can use the original column names to access the different columns of each row. Note for reference we could have done this same example in a completely vectorized way:\n",
        "```\n",
        "str_ser = df['Name']+' has '+ df['N_moons'].astype(str) + ' moons.'\n",
        "print('\\n'.join(str_col.values))\n",
        "```\n",
        "In the above example, I make a new Series by adding the different strings together. Notice the use of the `Series.astype(str)` function to turn the integers into strings. Then, I use the join function to turn the Series of strings into one long string, where the original strings are separated by a return character. Thus, we have reproduced the output of the previous for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKA8rBnHZOzZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzK_u7UEOi"
      },
      "source": [
        "### 1.3.3 Using Other People's Data\n",
        "\n",
        "Over this module we've been hinting at the fact that you will never create your own DataFrame from scratch in practical situations. Almost all data you analyze with Pandas will either be data you get from somewhere else or data that you create from some complicated simulation. In other words, you need to know how to take files of data and create a DataFrame out of it.\n",
        "\n",
        "A common format for data is called a CSV file for comma separated values. In a CSV file, each row contains the same amount of numbers, each separated by a comma. It's possible to use other \"separators\" as well. A commonly used one is whitespace, where some number of whitespaces (whether tabs or spaces) are used to separate different values in a row.\n",
        "\n",
        "One of the main ways to read such files in Pandas is to use `pd.read_csv()`.\n",
        "\n",
        "```\n",
        "filename = 'path/to/file' # whatever the path to the file is\n",
        "\n",
        "df = pd.read_csv(filename,...)\n",
        "```\n",
        "There are several parameters for this function that merit discussion.\n",
        "\n",
        "\n",
        "*   `sep`/`delimiter`: The separator to use. Defaults to `sep=','`. Another useful one is whitespace, which would be `sep='\\s+'`.\n",
        "*   `header`: Often, files will include a first row with a list of names. In this is the case, `header=0` will adopt those column names. \n",
        "*   `names`: If your file doesn't contain a header (or you don't like the names provided), you can supply your own list of names. If you pass a header in the option above, this parameter will be overwritten, so remember to use either `header` or `names`.\n",
        "*   `usecols`: If you don't want to read all the columns of a file (e.g. because of memory constraints) then you can pass a subset of columns to use. For example, if you wanted the first two and the fifth column only, then you'd use `usecols=[0,1,4]`.\n",
        "*   `comment`: It's common for files to be prepended with some number of lines telling you what the file contains and additional details. These lines will begin with some character or combination of characters so that when the computer reads them it's easy to tell which lines to ignore. One common choice is `comment=#`.\n",
        "\n",
        "Another important thing to keep in mind: the real world is typically much messier than we'd like. As a result, we don't always have all the information we want. So, it's common for real world data to have a lot of missing values. `read_csv()` will take care of this by filling in the missing values with `np.nan`. Consider these two rows from the data you'll use in the challenge problem:\n",
        "```\n",
        "324,EPIC 201427007 b,EPIC 201427007,1,1,1,Transit,2021,0.72091000,,1.500,0.134,,,,,,0,5633.00,0.94,,706.3910000\n",
        "325,EPIC 201497682 b,EPIC 201497682,1,1,1,Transit,2019,2.13174000,,0.692,0.062,,,,,,0,,,0.78,253.0260000\n",
        "```\n",
        "Ignoring what these mean for now, notice that in the middle of both rows, there are a lot of commas in a row. These correspond to five columns where either the data doesn't exist for one reason or another. Also, notice that the second row is shorter because it's missing two more values than the first row. When values are missing, they will be filled in with `np.nan`.\n",
        "\n",
        "### 1.3.4 Practice Problem 3: Binaries\n",
        "\n",
        "Redo the problem from the NumPy section, but this time using Pandas. This time, the file will be a csv.\n",
        "\n",
        "Also, additional tasks:\n",
        "1. Make a scatter plot of mass vs k-type\n",
        "2. Histograms of binary total mass, mass ratio (smaller mass over larger mass), and the masses of all the individual stars.\n",
        "3. Histogram of eccentricities.\n",
        "4. Scatter plot of the total mass vs the mass ratio.\n",
        "5. Anything else you may think of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pADNdPQ2PnSS"
      },
      "outputs": [],
      "source": [
        "# Running in Google Colab? Run this cell\n",
        "!wget https://raw.githubusercontent.com/CIERA-Northwestern/REACHpy/main/Module_3/data/binaries.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdZmdDaIPq0y"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8QvK4-INOI2G"
      },
      "source": [
        "### 1.3.5 Practice Problem 4: Potentially Hazardous Asteroids Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMxpsa1_LwmJ"
      },
      "outputs": [],
      "source": [
        "# Running in Google Colab? Run this cell\n",
        "!wget https://raw.githubusercontent.com/CIERA-Northwestern/REACHpy/main/Module_3/data/PHA_data.csv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this problem, you will look at a dataset describing Potentially Hazardous Asteroids (PHAs) which are cataloged by the Minor Planets Center, hosted at the Harvard-Smithsonian Center for Astrophysics. You will find out why these are called Potentially Hazardous.\n",
        "\n",
        "The original version of this data can be found [here](https://www.minorplanetcenter.net/data). The version supplied here has been reformatted for easier reading by `pd.read_csv()`. Note that the Center updates this data frequently, so the version here, made in early 2023, may be wildly inaccurate.\n",
        "\n",
        "The first thing to do is to open the file and look at the contents. Notice that the first line of the file contains a description of the columns. Therefore, it would be a good idea to use the keyword argument `header=0`. Similarly, the first column of data is an index, so you can choose to use the keyword argument `index=0`. Finally, the last column contains a bunch of dates. In order to make sure pandas parses this data correctly, we can use the keyword argument `parse_dates=['Date of last observation']`. In the cell below, put this all together and read the file using `pd.read_csv()`. Print the dataframe and inspect it to make sure nothing is out of the ordinary. Report the total number of rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DwWlobqUXI3"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you've read in the data, we can see where these objects get their name. Below, add two columns, one for the pericenter distance and one for the apocenter distance from the Sun in AU. Name these columns `'peri'` and `'apo'`. The equation for pericenter is $r_{\\rm peri} = a(1-e)$ and for apocenter, $r_{\\rm apo} = a(1+e)$. Note that the semimajor axis in the file is already in AU, so there is no unit conversion needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6HNgynKfEeF"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, make two histograms, one for the pericenter and one for the apocenter of each object. Add additional lines using `plt.axvline()` representing the semimajor axis of the inner planets. Optionally, also add dashed lines around these solid lines corresponding to the pericenter and apocenter of the inner planets and Jupiter. If you can, add shading to the region in between the dashed lines using `plt.axvspan`.\n",
        "\n",
        "The documentation of `plt.axvline()` can be found [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvline.html#matplotlib.pyplot.axvline). The documentation of `plt.axvspan()` can be found [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.axvspan.html). Here's an example for how you'd do this for Mars. Note I am shading the region but without the dashed lines for the pericenter and apocenter. When doing this for multiple planets, it would be easier to do so with a for loop.\n",
        "```\n",
        "Mars_SMA = 1.524 # AU\n",
        "Mars_ecc = 0.0934\n",
        "\n",
        "Mars_peri = Mars_SMA*(1 - Mars_ecc)\n",
        "Mars_apo = Mars_SMA*(1 + Mars_ecc)\n",
        "\n",
        "plt.axvline(x = Mars_SMA, color='g')\n",
        "plt.axvspan(Mars_peri, Mars_apo, facecolor='g', alpha=0.5)\n",
        "```\n",
        "\n",
        "Do you notice anything potentially alarming? In order to make this a more quantitative claim, how many objects in the dataset have a pericenter smaller than Earth's semimajor axis AND an apocenter larger than Earth's semimajor axis? Repeat this for each of the inner planets and Jupiter.\n",
        "\n",
        "You should notice that some of these objects do not seem to have Earth-crossing orbits. Why do you think they're on this list?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W71jSw9fEpD"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make an additional histogram showing the dates of last observation. Since this is datetime data, which pandas handles internally quite efficiently, you can use the method `df['Date of Last Observation'].hist()` to do this quickly. Look at the range on the x-axis. Is this concerning? \n",
        "\n",
        "In order to make a quantitative claim, how many of these have not been observed within the past 1, 5, and 10 years?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Optional***\n",
        "\n",
        "We can make a better version of the first two histograms in a way that's visually more intuitive. For the purposes of the code below, I'll assume you named your Dataframe `df`.\n",
        "\n",
        "First, let's reorder the rows to sort by pericenter and apocenter values in ascending order.\n",
        "```\n",
        "df = df.sort_values(by=['peri', 'apo'])\n",
        "```\n",
        "Next, since the dataset is so large, let's select only a subset of the rows to visualize. However, we still want to show the whole extent of the dataset, so choosing the first chunk of the data would not be a good option. Instead, let's choose every nth row. For this case, I'll use `n=40`, though you should feel free to play around with this number. I will make this a variable so it is easy to change later.\n",
        "```\n",
        "nth_row = 40\n",
        "df_reduced = df.iloc[::nth_row, :]\n",
        "```\n",
        "One more step. In order to make our plot look nice, let's also reset the index. Optionally, you can add the keyword argument `drop=True`. If you don't (I will not), then your new dataframe will add the old indices as a new column.\n",
        "```\n",
        "df_reduced = df_reduced.reset_index()\n",
        "```\n",
        "Now, we can get to plotting. What we will do is use the index value as the y-value so that each object gets its own line. For the x-values, we will use the semimajor axis. We will also include the extent of the orbit by plotting the pericenter and apocenter using errorbar.\n",
        "```\n",
        "xdata = df_reduced['Semimajor Axis (AU)'].values\n",
        "ydata = df_reduced.index\n",
        "xerr_low = xdata - df_reduced['peri'].values\n",
        "xerr_high = df_reduced['apo'].values - xdata\n",
        "xerr = (xerr_low, xerr_high)\n",
        "plt.errorbar(xdata, ydata, xerr = xerr, linestyle='')\n",
        "```\n",
        "As before, also added some vertical lines and shaded regions representing the inner planets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Section1_Intro_to_Pandas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
