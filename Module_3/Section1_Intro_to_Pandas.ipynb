{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Section1_Intro_to_Pandas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1. Intro to Pandas\n",
        "\n",
        "So far, we have used Python lists as well as Numpy arrays to store and manipulate data, each of which have their place in a simple Python program. But there's an excellent (and very popular) Python package called Pandas that greatly facilitates handling comma separated value (CSV) files and other types of data files. The basic tool in pandas is the DataFrame, which you can think of as a large table with built-in functions to process rows and columns, read and write to many different formats on disk, and interact with other DataFrames. One key advantage of pandas over other types of data storage is that it can easily handle data of many different types, including strings and numbers.\n",
        "\n",
        "## 1.1 Getting Started\n",
        "\n",
        "First things first, we need to import the package. We should also import NumPy for other useful functions and matplotlib for when we plot\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "```"
      ],
      "metadata": {
        "id": "rgFQ0jMkuMa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdPe93y5uB-r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 The DataFrame\n",
        "\n",
        "There are many ways to create DataFrames in Pandas. Here, we will use one method that involves lists. In this case, each row is its own list, much like with a NumPy 2D array.\n",
        "\n",
        "First, let's define some familiar looking data.\n",
        "\n",
        "```\n",
        "planets = [['Mercury', 0.0553, 0.4, 0],\n",
        "           ['Venus', 0.815, 0.7, 0],\n",
        "           ['Earth', 1.0, 1.0, 1],\n",
        "           ['Mars', 0.107, 1.5, 2],\n",
        "           ['Jupiter', 317.8, 5.2, 80],\n",
        "           ['Saturn', 95.2, 9.5, 83],\n",
        "           ['Uranus', 14.5, 19.2, 27],\n",
        "           ['Neptune', 17.1, 30.1, 14]]\n",
        "\n",
        "colnames = ['Name','Mass','Distance','N_moons']\n",
        "# Units: None, Earth Masses, AU, None\n",
        "```\n",
        "\n",
        "Take note of a few things here. \n",
        "\n",
        "First, unlike in NumPy arrays, where you can only have one type (usually int or float), each row I've defined here mixes three different types of data: string, float, and int. This is one of the advantages of Pandas: different columns are allowed (and encouraged) to have different datatypes, depending on the usecase. In particular, one useful function we will not use here is the [time/datetime](https://pandas.pydata.org/docs/user_guide/timeseries.html) datatype.\n",
        "\n",
        "Second, notice that I have also defined a second list to use as names for the columns. This is a huge feature of the design philosophy of the DataFrame. While a NumPy 2D array can be thought of as a huge matrix of numbers, a DataFrame really is a table. Note that I could have put the units in the column names. However, that can quickly become very unwieldy for reasons you will see later. \n",
        "\n",
        "Now, let's make the dataframe and print it out.\n",
        "```\n",
        "df = pd.DataFrame(planets, columns=colnames)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "In the second cell, just try running this and see how the output may look different. This distinction is only relevant when working in a Jupyter notebook. When working with large amounts of data with Python scripts, we'd essentially never need to display our DataFrames in either manner.\n",
        "```\n",
        "df\n",
        "```"
      ],
      "metadata": {
        "id": "DhOD9Xto_DNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DD_ASdZXEnUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WUlfOT4_gV8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Accessing Rows and Cells\n",
        "\n",
        "First, let's talk about how to access individual rows and cells. For this, we will use the method `df.loc`, where you can replace df with the name of your dataframe. \n",
        "\n",
        "You probably noticed in the previous example that each row is specified by an index. In the example above, we just had integer indices from 0 to 7. So, if we want to access the row for Earth, we can just execute\n",
        "```\n",
        "df.loc[2]\n",
        "```\n",
        "Note that some Pandas DataFrames you encounter may be indexed differently. For instance, the indices may not correspond to their row number at all. In fact, you can even find tables where the indices are strings. When using `df.loc` you must remember to use the label given.\n",
        "\n",
        "We can illustrate this quite well by showing how to access a specific cell. We simply specify both the index (as above) and the column name. So, if we wanted to get the mass of Venus, for example, we'd execute\n",
        "```\n",
        "df.loc[2,'Mass']\n",
        "```\n",
        "Try this yourself below"
      ],
      "metadata": {
        "id": "bY4XVvbgfhsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RYPeOdeciGKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we can also use this functionality with `.loc` to \n",
        "change the values of individual cells, but in practice when working with data we didn't create ourselves, there is no reason to do so.\n",
        "\n",
        "### 1.1.3 Accessing Columns\n",
        "\n",
        "Accessing individual rows and cells is actually less useful than you'd think since each row only represents one data point. Usually, when we're using tables, we're actually more interested in the statistics of many data points. The DataFrame structure represents this, because it's very easy to get different columns. For example, if we want the different masses,\n",
        "```\n",
        "df['Mass']\n",
        "```\n",
        "If we wanted multiple columns, we enter a list of column names instead of just one, e.g.,\n",
        "```\n",
        "df[['Mass','Distance']]\n",
        "```\n",
        "Note this is why I chose not to include units in column names. Since we need to write out the column names, adding the units can make the code harder to parse. Instead, it can be better, depending on your preferences, to state clearly in a comment what units are being used."
      ],
      "metadata": {
        "id": "sRxCRggdiZbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "evfVfW2CiN92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.4 df.values\n",
        "When using some functions, like `plt.plot()`, we don't want the full Pandas DataFrame. Using `df.values` will extract the values and return a NumPy array.\n",
        "```\n",
        "df.values # returns a 2D array\n",
        "df.loc[2].values # returns a 1D array of the third row\n",
        "df['Mass'].values # returns a 1D array of the Mass column\n",
        "```"
      ],
      "metadata": {
        "id": "KtFBqIS7po2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L9I5z5sGr1he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Making Changes to a DataFrame\n",
        "### 1.2.1 Adding New Columns from Data\n",
        "\n",
        "Let's say we get more data from a different source. We want to add it to our DataFrame. We can do so like this.\n",
        "```\n",
        "radii = [0.383, 0.949, 1, 0.532, 11.21, 9.45, 4.01, 3.88] # in Earth radii \n",
        "\n",
        "df['Radius'] = radii\n",
        "```\n",
        "\n",
        "One big note: **your list or list-like object with the new data must be in the same order as the rows in your DataFrame. If they aren't, your data will be incorrect. This is extremely important.**\n",
        "\n",
        "Add this new column below. It will be used in the next section."
      ],
      "metadata": {
        "id": "_jFSOPt0r0oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "el9h05K5sRGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Adding New Columns from Other Columns\n",
        "\n",
        "It's important to remember that columns are essentially NumPy arrays with extra stuff. As a result, we can do mathematical operations on columns just like we do with arrays. We can leverage this to create new columns. For example, let's get the density of the different planets. Remember that density is mass over volume. For this example, let's get the density in SI units. As a result, we'll also need to convert the masses and radii to SI units\n",
        "```\n",
        "m_e = 5.9722e24 # kg\n",
        "r_e = 6.371e6 # m, using the average radius\n",
        "df['Density'] = m_e*df['Mass']/((4/3)*np.pi*(r_e*df['Radius'])**3)\n",
        "```\n",
        "\n",
        "Now that you've done that, what do you notice about the densities of the outer planets versus the inner planets? Compare these densities to the densities of various materials in this [wikipedia page](https://en.wikipedia.org/wiki/Density#Densities). Which materials are these densities most similar to?\n",
        "\n",
        "What you should find is that the inner planets are rocky, so their densities are more comparable to metals, while the gas giants have densities that are closer to that of water."
      ],
      "metadata": {
        "id": "OnyUwre6sRp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hhQvBYew5Y2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 Practice Problem 1: Kepler's Law\n",
        "\n",
        "Using Kepler's Third Law, make another column that corresponds to the orbital period for each planet **in years.** Once again, Keplers Third Law is \n",
        "\n",
        "$$T = 2\\pi \\sqrt{\\frac{a^3}{G(M_1 + M_2)}}$$\n",
        "\n",
        "For the case of solar system planets, you may make the approximation that $M_1 + M_2 \\approx M_\\odot \\approx 2e30$ kg. \n",
        "\n",
        "You can calculate by hand how many seconds are in a year, but note a year technically has 365.25 days!\n",
        "\n",
        "Finally, you'll need to know that 1 AU is defined as exactly 149597870700 m."
      ],
      "metadata": {
        "id": "KTndNlqg0b1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bMpWTjUhDBzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 Practice Problem 2: Equilibrium Temperature\n",
        "\n",
        "The [equilibrium temperature](https://en.wikipedia.org/wiki/Planetary_equilibrium_temperature) of a planet is defined as the temperature at which the power supplied by its star (due to solar radiation) is equal to the power emitted by the planet (due to blackbody radiation). Without getting too much into the details, the formula (in units of Kelvin) is as follows:\n",
        "\n",
        "$$ T_{\\rm eq} =  \\left( \\frac{L (1 - A_B)}{16\\sigma\\pi a^2} \\right)^{1/4}$$\n",
        "\n",
        "Here, L is the luminosity of the Sun (3.828e26 Watts), a is the semimajor axis (same as the \"distance\" column), $\\sigma =$ 5.67e-8 (SI units) is the Stefan-Boltzmann constant, and $A_B$ is something called the [Bond Albedo](https://en.wikipedia.org/wiki/Bond_albedo).\n",
        "\n",
        "Your task:\n",
        "1. Create a new column for the Bond albedo of each planet, taking the data from the linked wikipedia article\n",
        "2. Create a new column for the equilibrium temperature using the formula above. \n",
        "3. Create a new column for the effective/surface temperature of each planet using the values listed below. \n",
        "4. Create a scatter plot of the predicted equilibrium temperature versus the actual surface temperature. Show the 1-to-1 line using a dotted line for comparison. After reading the first linked wikipedia article, can you think of at least one reason why they may be different? (Hint: you've definitely learned about one of them in other science classes or the news)\n",
        "\n",
        "```\n",
        "Teff = [412.5, 737, 288, 215, 124.4, 95, 59.1, 59.3]\n",
        "```\n",
        "Values taken from de Pater and Lissauer (2010). Note Mercury's is the average of 100 and 725\n",
        "\n"
      ],
      "metadata": {
        "id": "N-weF6RUDCqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "quCOQWQmI_HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Intermediate Pandas\n",
        "\n",
        "### 1.3.1 Boolean Indexing with DataFrames\n",
        "\n",
        "When we take a look at data, it can happen that we are only interested in a subset of the rows based on one or more conditions. Luckily for us, this is almost exactly identical to doing the same thing with NumPy arrays! The difference is that we use the column names instead of their indices. For example, let's get only the planets that have moons.\n",
        "\n",
        "```\n",
        "df[df['N_moons'] > 0]\n",
        "```\n",
        "\n",
        "Just like before, we can chain different things together. For example, let's get only the planets that have moons but are further than 5 au from the Sun.\n",
        "\n",
        "```\n",
        "df[(df['N_moons'] > 0) & (df['Distance] > 5)]\n",
        "```\n",
        "\n",
        "As a reminder, here is a list of the logical operators you can use:\n",
        "\n",
        "```\n",
        "== # equal to\n",
        "!= # not equal to\n",
        "<= # less than or equal to\n",
        "<  # less than\n",
        ">= # greater than or equal to\n",
        ">  # greater than\n",
        "\n",
        "& # logical and\n",
        "| # logical or\n",
        "~ # logical not\n",
        "```\n"
      ],
      "metadata": {
        "id": "-yQ-Rr3-I_cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-gFDgRL4UEAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 df.iterrows()\n",
        "\n",
        "There are extremely rare cases where you will find it is necessary to iterate over the rows of a dataframe. These cases should be extremely rare. In almost every instance, there is some way of doing this using vectorized functions. In fact, the official documentation has a warning against doing this:\n",
        "\n",
        "> Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].\n",
        "\n",
        "In any case, there are times when this may be necessary. In those cases, we can use the function `df.iterrows()` which allows us to iterate over all the rows in the dataframe. In the following example\n",
        "```\n",
        "for i,row in df.iterrows():\n",
        "    print(row['Name'],'has',row['N_moons'],'moons.')\n",
        "```\n",
        "In this example, `df.iterrows()` returns an index,series pair for each row in the dataframe. Since each row is returned as a Pandas Series, we can use the original column names to access the different columns of each row. Note for reference we could have done this same example in a completely vectorized way:\n",
        "```\n",
        "str_ser = df['Name']+' has '+ df['N_moons'].astype(str) + ' moons.'\n",
        "print('\\n'.join(str_col.values))\n",
        "```\n",
        "In the above example, I make a new Series by adding the different strings together. Notice the use of the `Series.astype(str)` function to turn the integers into strings. Then, I use the join function to turn the Series of strings into one long string, where the original strings are separated by a return character. Thus, we have reproduced the output of the previous for loop."
      ],
      "metadata": {
        "id": "5KomXNChLeix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MKA8rBnHZOzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Using Other People's Data\n",
        "\n",
        "Over this module I've been hinting at the fact that you will never create your own DataFrame from scratch in practical situations. Almost all data you analyze with Pandas will either be data you get from somewhere else or data that you create from some complicated simulation. In other words, you need to know how to take files of data and create a DataFrame out of it.\n",
        "\n",
        "A common format for data is called a CSV file for comma separated values. In a CSV file, each row contains the same amount of numbers, each separated by a comma. It's possible to use other \"separators\" as well. A commonly used one is whitespace, where some number of whitespaces (whether tabs or spaces) are used to separate different values in a row.\n",
        "\n",
        "One of the main ways to read such files in Pandas is to use `pd.read_csv()`.\n",
        "\n",
        "```\n",
        "filename = 'path/to/file' # whatever the path to the file is\n",
        "\n",
        "df = pd.read_csv(filename,...)\n",
        "```\n",
        "There are several parameters for this function that merit discussion.\n",
        "\n",
        "\n",
        "*   `sep`/`delimiter`: The separator to use. Defaults to `sep=','`. Another useful one is whitespace, which would be `sep='\\s+'`.\n",
        "*   `header`: Often, files will include a first row with a list of names. In this is the case, `header=0` will adopt those column names. \n",
        "*   `names`: If your file doesn't contain a header (or you don't like the names provided), you can supply your own list of names. If you pass a header in the option above, this parameter will be overwritten, so remember to use either `header` or `names`.\n",
        "*   `usecols`: If you don't want to read all the columns of a file (e.g. because of memory constraints) then you can pass a subset of columns to use. For example, if you wanted the first two and the fifth column only, then you'd use `usecols=[0,1,4]`.\n",
        "*   `comment`: It's common for files to be prepended with some number of lines telling you what the file contains and additional details. These lines will begin with some character or combination of characters so that when the computer reads them it's easy to tell which lines to ignore. One common choice is `comment=#`.\n",
        "\n",
        "Another important thing to keep in mind: the real world is typically much messier than we'd like. As a result, we don't always have all the information we want. So, it's common for real world data to have a lot of missing values. `read_csv()` will take care of this by filling in the missing values with `np.nan`. Consider these two rows from the data you'll use in the practice problem:\n",
        "```\n",
        "324,EPIC 201427007 b,EPIC 201427007,1,1,1,Transit,2021,0.72091000,,1.500,0.134,,,,,,0,5633.00,0.94,,706.3910000\n",
        "325,EPIC 201497682 b,EPIC 201497682,1,1,1,Transit,2019,2.13174000,,0.692,0.062,,,,,,0,,,0.78,253.0260000\n",
        "```\n",
        "Ignoring what these mean for now, notice that in the middle of both rows, there are a lot of commas in a row. These correspond to five columns where either the data doesn't exist for one reason or another. Also, notice that the second row is shorter because it's missing two more values than the first row. When values are missing, they will be filled in with `np.nan`.\n",
        "\n",
        "### 1.3.4 Practice Problem 3: Binaries\n",
        "\n",
        "Redo the problem from the NumPy section, but this time using Pandas. This time, the file will be a csv.\n",
        "\n",
        "Also, additional tasks:\n",
        "1. Make a scatter plot of mass vs k-type\n",
        "2. Histograms of binary total mass, mass ratio (smaller mass over larger mass), and the masses of all the individual stars.\n",
        "3. Histogram of eccentricities.\n",
        "4. Scatter plot of the total mass vs the mass ratio.\n",
        "5. Anything else you may think of."
      ],
      "metadata": {
        "id": "hkAzK_u7UEOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running in Google Colab? Run this cell\n",
        "!wget https://raw.githubusercontent.com/CIERA-Northwestern/REACHpy/main/Module_3/data/binaries.csv"
      ],
      "metadata": {
        "id": "pADNdPQ2PnSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TdZmdDaIPq0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.5 Practice Problem 4: Exoplanet Data\n",
        "\n",
        "Before loading it into Python, open up the file named `exoplanet_data.csv` and take a quick look at its contents. The file starts with many lines of comments. In particular, there are descriptions of each of the columns included in the file. Most of them are somewhat clear, but there are two that aren't which you should ignore when doing the problem, which I will describe briefly.\n",
        "*    `loc_rowid`: The table in the file doesn't actually correspond to the whole database, so the values in this column tell you the original row indices in the full dataset.\n",
        "*    `default_flag`: For each planet, there can be more than one \"solution\" for the set of observations. This flag indicates whether the row is the \"default\" solution. I chose only the subset of rows where this value is 1, corresponding to the default solutions. \n",
        "\n",
        "One important thing to mention is that I downloaded a version of this data excluding the error bars. In reality, all these measurements come with some amount of uncertainty.\n",
        "\n",
        "Now, open the file using `pd.read_csv`.\n",
        "*   The file is already comma-separated, so no need to pass a separator\n",
        "*   You can use either `header=0` or pass your own list of names if you don't like the ones in the file.\n",
        "*   You can optionally exclude the two columns I mentioned above with `usecols` but it's probably more trouble than it's worth.\n",
        "*   You want to ignore the lines of comments, so make sure to use `comment=#`.\n",
        "\n",
        "Your main tasks:\n",
        "1.   For starters, how many rows does the DataFrame contain? This translates to the total number of confirmed exoplanets.\n",
        "2.   How many different exoplanet detection methods are there? You can get the unique values of a column by using the method `df['Columnname'].unique()`.\n",
        "3.   Make a cumulative histogram showing how the number of known exoplanets has grown over time. Make a second version where you break this down by detection method. Do you notice any years with very drastic increases?\n",
        "\n",
        "Optional tasks if you have time:\n",
        "*   Now, make some histograms of some of the physical values, including exoplanet radius, semimajor axis, host star mass, etc. For your histogram of the exoplanet mass, restrict yourself to the rows where `bl_pmassprov=='Mass'` instead of `Msini`. Do you notice anything interesting? For example, you should find that most host stars will have masses around 1 solar mass. This is because most exoplanet surveys have purposefully targeted stars similar to our own Sun. Other things to look into: Are there more planets further away than close to us? Does planetary radius or mass seem to peak at any specific values?\n",
        "*   Kepler derived his third law from actual observations of the solar system planets where he plotted the semimajor axis vs the orbital period. Try doing this with log-log axes. When you do this for the solar system planets, you get a straight line. What do you notice in this case?\n",
        "*   Is there any correlation between distance from the star and planetary mass or radius? Make some scatter plots to investigate this.\n",
        "*   From the planetary mass and radius, you can get the planetary density. Make a histogram of this. What do you notice?\n",
        "*   Anything else you can think of! Try stuff out.\n",
        "\n",
        "Keep in mind that some of the conclusions you draw here might not be true. Observations are prone to some level of bias, also known as \"selection effects\". For example, it's very hard to directly image exoplanets that are small and close to their host star, so the method of direct imaging is biased towards larger exoplanets and larger distances. Each detection method is biased in some way, so take some of your conclusions with a grain of salt."
      ],
      "metadata": {
        "id": "8QvK4-INOI2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running in Google Colab? Run this cell\n",
        "!wget https://raw.githubusercontent.com/CIERA-Northwestern/REACHpy/main/Module_3/data/exoplanet_data.csv"
      ],
      "metadata": {
        "id": "rMxpsa1_LwmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2DwWlobqUXI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f6HNgynKfEeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0W71jSw9fEpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}